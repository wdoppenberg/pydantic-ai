interactions:
  - request:
      body: grant_type=%5B%27refresh_token%27%5D&client_id=%5B%27764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com%27%5D&client_secret=%5B%27scrubbed%27%5D&refresh_token=%5B%27scrubbed%27%5D
      headers:
        accept:
          - "*/*"
        accept-encoding:
          - gzip, deflate
        connection:
          - keep-alive
        content-length:
          - "268"
        content-type:
          - application/x-www-form-urlencoded
      method: POST
      uri: https://oauth2.googleapis.com/token
    response:
      headers:
        alt-svc:
          - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
        cache-control:
          - no-cache, no-store, max-age=0, must-revalidate
        content-length:
          - "1419"
        content-type:
          - application/json; charset=utf-8
        expires:
          - Mon, 01 Jan 1990 00:00:00 GMT
        pragma:
          - no-cache
        transfer-encoding:
          - chunked
        vary:
          - Origin
          - X-Origin
          - Referer
      parsed_body:
        access_token: scrubbed
        expires_in: 3599
        id_token: eyJhbGciOiJSUzI1NiIsImtpZCI6IjgyMWYzYmM2NmYwNzUxZjc4NDA2MDY3OTliMWFkZjllOWZiNjBkZmIiLCJ0eXAiOiJKV1QifQ.eyJpc3MiOiJodHRwczovL2FjY291bnRzLmdvb2dsZS5jb20iLCJhenAiOiI3NjQwODYwNTE4NTAtNnFyNHA2Z3BpNmhuNTA2cHQ4ZWp1cTgzZGkzNDFodXIuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJhdWQiOiI3NjQwODYwNTE4NTAtNnFyNHA2Z3BpNmhuNTA2cHQ4ZWp1cTgzZGkzNDFodXIuYXBwcy5nb29nbGV1c2VyY29udGVudC5jb20iLCJzdWIiOiIxMDY1Njg0NzQzMTU3NzkyMTI1NTkiLCJoZCI6InB5ZGFudGljLmRldiIsImVtYWlsIjoibWFyY2Vsb0BweWRhbnRpYy5kZXYiLCJlbWFpbF92ZXJpZmllZCI6dHJ1ZSwiYXRfaGFzaCI6ImlyckNRNE00c0Z0Z2dfS2hRTVNjekEiLCJpYXQiOjE3NDM0MTM3NzcsImV4cCI6MTc0MzQxNzM3N30.BAvb4TlcIoYcQODNLFqwtUQoSNJJbpAR0lk2OyFxXK9rSZ7m1e1_Dp1O4ApxPUS7f_NX34eSCuDJN2IXgh8VBv4k3IhI7CbMydYeqXuwlbgOOp1Z0farGEKneU1M7TvdngigAJ9wT-2LHjKd_GEcGau-CUvzXpcT1IOnNNyXGVqtuGmEfcw5jjPkKJNECUheeNHE3zeImatTstOLuKmI1ZK-etl41l3poSNuQkZkrbQ80Vst8BdT-b1tnJnXP1KGATBIamDy99OOiB9a7a9m_ikXYEyN91yR76DYot3hpDPlOX0H9hF-BOSqoOtlSS2TMBkMvFiiYWjID1e_9VlNUg
        scope: https://www.googleapis.com/auth/cloud-platform https://www.googleapis.com/auth/userinfo.email openid https://www.googleapis.com/auth/sqlservice.login
        token_type: Bearer
      status:
        code: 200
        message: OK
  - request:
      headers:
        accept:
          - "*/*"
        accept-encoding:
          - gzip, deflate
        connection:
          - keep-alive
        content-length:
          - "249"
        content-type:
          - application/json
        host:
          - aiplatform.googleapis.com
      method: POST
      parsed_body:
        contents:
          - parts:
              - text: What is the main content of this URL?
              - fileData:
                  file_uri: gs://pydantic-ai-dev/Gemini_1_5_Pro_Technical_Report_Arxiv_1805.pdf
                  mime_type: application/pdf
            role: user
        generationConfig: {}
      uri: https://aiplatform.googleapis.com/v1beta1/projects/pydantic-ai/locations/global/publishers/google/models/gemini-2.0-flash:generateContent
    response:
      headers:
        alt-svc:
          - h3=":443"; ma=2592000,h3-29=":443"; ma=2592000
        content-length:
          - "1837"
        content-type:
          - application/json; charset=UTF-8
        transfer-encoding:
          - chunked
        vary:
          - Origin
          - X-Origin
          - Referer
      parsed_body:
        candidates:
          - avgLogprobs: -0.4750063268149771
            content:
              parts:
                - text:
                    "The URL leads to a research paper titled \"Gemini 1.5: Unlocking multimodal understanding across millions
                    of tokens of context\".  \n\nThe paper introduces Gemini 1.5 Pro, a new model in the Gemini family. It's described
                    as a highly compute-efficient multimodal mixture-of-experts model.  A key feature is its ability to recall and
                    reason over fine-grained information from millions of tokens of context, including long documents and hours
                    of video and audio.  The paper presents experimental results showcasing the model's capabilities on long-context
                    retrieval tasks, QA, ASR, and its performance compared to Gemini 1.0 models. It covers the model's architecture,
                    training data, and evaluations on both synthetic and real-world tasks.  A notable highlight is its ability to
                    learn to translate from English to Kalamang, a low-resource language, from just a grammar manual and dictionary
                    provided in context.  The paper also discusses responsible deployment considerations, including impact assessments
                    and mitigation efforts.\n"
              role: model
            finishReason: STOP
        createTime: "2025-05-31T21:34:43.658150Z"
        modelVersion: gemini-2.0-flash
        responseId: 83U7aOaVKNO8nvgPkuvc0QY
        usageMetadata:
          candidatesTokenCount: 205
          candidatesTokensDetails:
            - modality: TEXT
              tokenCount: 205
          promptTokenCount: 19875
          promptTokensDetails:
            - modality: TEXT
              tokenCount: 9
            - modality: DOCUMENT
              tokenCount: 19866
          totalTokenCount: 20080
          trafficType: ON_DEMAND
      status:
        code: 200
        message: OK
version: 1
